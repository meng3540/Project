# Parallel Programming Project Farid-Sabir
# Discussion on Problem Statement
Real-time object detection is a cornerstone task in modern artificial intelligence applications, particularly in domains that demand immediate contextual awareness such as autonomous vehicles, robotic navigation, security systems, and smart manufacturing. Traditionally, these compute-heavy tasks have relied on cloud infrastructure due to its raw processing power. However, this approach introduces latency, bandwidth limitations, and potential privacy concerns. The evolving trend now favors embedded edge devices with built-in accelerators that enable AI inference to occur locally, closer to where data is generated. This shift toward edge computing brings substantial benefits in responsiveness, reliability, and security. The central challenge addressed in this project is to prototype and evaluate a real-time object detection pipeline running directly on an embedded GPU platform. Specifically, the aim is to demonstrate how the NVIDIA Jetson Orin Nano, a compact and power-efficient device equipped with a CUDA-capable GPU, can be used to accelerate image-based AI inference using optimized pre-trained models and parallel processing frameworks. This project not only highlights the performance gains possible through hardware acceleration but also explores how such systems can be made efficient, reproducible, and scalable for future deployment in real-world edge environments.
# Initial Configuration and Setup of the Edge Device and the Software
The development environment was built around the NVIDIA Jetson Orin Nano, chosen for its balance of compact form factor and strong GPU capabilities designed specifically for edge AI workloads. The board was flashed with the JetPack SDK, which includes the Ubuntu 20.04 operating system along with pre-configured support for CUDA, cuDNN, TensorRT, and other essential NVIDIA libraries. Python 3.10 was used as the primary programming language, and all dependencies were installed within a Python virtual environment to ensure modularity and environment isolation. PyTorch with CUDA support was verified through simple diagnostic scripts, and the jtop utility was used to monitor real-time GPU usage and memory allocation. To interface with the external world, a Logitech USB webcam was connected to the Jetson and verified using OpenCV, which was also used for video frame acquisition and initial image processing tasks such as real-time Gaussian blur. This setup provided a robust foundation for real-time visual data processing, ensuring that the hardware and software stack was correctly configured and GPU resources were accessible for later deep learning tasks.
# List of Possible Hardware, Software Frameworks, and Libraries That Can Be Used
The core of the project is the Jetson Orin Nano development board, which features an Ampere-based GPU architecture optimized for parallel computing. A standard USB webcam was used to provide continuous real-time image input, though other camera modules such as the Raspberry Pi Camera Module V2 or MIPI CSI-2 compatible sensors could also be integrated. On the software side, PyTorch was selected as the deep learning framework due to its dynamic computation graph, ease of use, and strong GPU integration. OpenCV was used for real-time image capture and pre-processing, with optional support for CUDA acceleration on applicable operations. Additional libraries considered for model deployment and optimization included TensorRT for runtime performance tuning, ONNX for interoperability between frameworks, and torchvision for accessing model architectures and dataset utilities. System monitoring and profiling were supported through tools like jtop and tegrastats, while project documentation and collaboration were managed using GitHub. These components form a flexible and powerful stack capable of handling high-throughput, low-latency AI inference workloads directly on the embedded system.
# Choice of Pre-trained Models and Rationale for Your Selection
The pre-trained model selected for this project was YOLOv5, specifically the yolov5s variant, developed and maintained by Ultralytics. YOLOv5 was chosen due to its well-balanced trade-off between detection accuracy and computational efficiency, which makes it particularly suitable for deployment on edge devices with limited resources. Among its available versions—nano, small, medium, large, and extra-large—the small model (yolov5s) was deemed optimal for the Jetson Orin Nano, delivering real-time inference performance without compromising significantly on precision. The model is trained on the COCO dataset and is capable of detecting a wide variety of everyday objects, which aligns with the general-purpose nature of the object detection task for this proof-of-concept. Another reason for selecting YOLOv5 was its seamless integration with PyTorch, allowing us to avoid complex model conversion pipelines and leverage native GPU acceleration with minimal configuration. In future iterations, the model can also be converted to ONNX format and optimized further using TensorRT, enabling additional performance gains. Overall, YOLOv5’s architecture, community support, and real-time capability made it an ideal candidate for the embedded inference task at hand.
